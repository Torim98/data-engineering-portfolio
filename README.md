# Data Engineering Portfolio: Batch-Processing Pipeline f√ºr Schach-Analysen

Dieses Repository beinhaltet die Implementierung einer **Batch-basierten Datenarchitektur** im Rahmen des Portfolioprojekts "Data Engineering". Ziel des Projekts ist der Aufbau einer containerisierten Pipeline, die Schachpartien (PGN-Format) einliest, bereinigt, aggregiert und in einem Dashboard visualisiert.

Als Datenbasis dient die [Lichess Open Database](https://database.lichess.org/).

## Architektur

Die Pipeline folgt einem Microservice-Ansatz und ist vollst√§ndig in **Docker** containerisiert. Sie implementiert eine "Medallion Architecture" (Bronze/Gold Layer) innerhalb eines lokalen Data Lakes.

![Architektur Flowchart](assets/data-engineering_flowchart_dark.drawio.png)

**Die Komponenten:**

1.  **Ingestion Service:**
    *   Liest komprimierte Rohdaten (`.pgn.zst`).
    *   Implementiert Chunking/Partitioning: Verarbeitet Daten in konfigurierbaren Batches, um den RAM-Verbrauch konstant niedrig zu halten.
	*   Der Prozess nutzt Python `multiprocessing` (ProcessPoolExecutor), um mehrere Quelldateien parallel auf allen verf√ºgbaren CPU-Kernen zu verarbeiten, was den Durchsatz signifikant erh√∂ht.
    *   Extrahiert Metadaten (Elo, Er√∂ffnung, Ergebnis) mittels `python-chess`.
    *   Speichert Rohdaten als partitionierte **Parquet**-Dateien (Bronze Layer).
2.  **Processing Service:**
    *   Wartet auf Abschluss der Ingestion.
    *   F√ºhrt Data Cleaning durch (Filterung ung√ºltiger Partien).
    *   Aggregiert Gewinnwahrscheinlichkeiten basierend auf Er√∂ffnungen (ECO-Codes).
    *   Speichert Ergebnisse im Gold Layer.
3.  **Dashboard Service:**
    *   Visualisiert die Ergebnisse mittels **Streamlit**.
    *   Erm√∂glicht explorative Datenanalyse im Browser.

---

## Technologie-Stack

*   **Infrastructure as Code:** Docker & Docker Compose
*   **Sprache:** Python 3.11 (Slim Images)
*   **Datenformat:** Apache Parquet (Snappy Compression)
*   **Libraries:** Pandas, PyArrow, Python-Chess, Zstandard
*   **Frontend:** Streamlit

---

## Installation & Ausf√ºhrung

Voraussetzung: **Docker Desktop** muss installiert sein und laufen.

### 1. Repository klonen
```bash
git clone https://github.com/Torim98/data-engineering-portfolio.git
cd data-engineering-portfolio
```

### 2. Testdaten herunterladen

Aus Gr√ºnden der Speicherplatzoptimierung sind die Rohdaten nicht im Repository enthalten.

1. Lade eine Beispieldatei von database.lichess.org herunter (f√ºr Tests empfehlen sich kleinere Dateien von 2013/2014).
2. Platziere die Dateien im Ordner `data/`.

Die Ordnerstruktur sollte so aussehen:

```Text
/portfolio-chess-analytics
  ‚îú‚îÄ‚îÄ dashboard/
  ‚îú‚îÄ‚îÄ data/
  ‚îÇ   ‚îî‚îÄ‚îÄ lichess_sample1.pgn.zst
  ‚îÇ   ‚îî‚îÄ‚îÄ lichess_sample2.pgn.zst
  ‚îú‚îÄ‚îÄ env/
  ‚îú‚îÄ‚îÄ ingestion/
  ‚îú‚îÄ‚îÄ logs/
  ‚îú‚îÄ‚îÄ processing/
  ‚îî‚îÄ‚îÄ docker-compose.yml
```

### 3. Pipeline starten

F√ºhre folgenden Befehl im Hauptverzeichnis aus:

```Bash
docker compose up --build
```

### 4. Dashboard √∂ffnen

Sobald die Pipeline durchgelaufen ist, ist das Dashboard unter folgender URL erreichbar:

üëâ http://localhost:8501

![Dashboard Vorschau](assets/dashboard_preview.png)

---

## Konfiguration √ºber Environment-Files

Die Konfiguration aller Services erfolgt √ºber dedizierte `.env`-Dateien und nicht direkt in der `docker-compose.yml`.

```Text
/env
  ‚îú‚îÄ‚îÄ ingestion.env
  ‚îú‚îÄ‚îÄ processing.env
  ‚îî‚îÄ‚îÄ dashboard.env
```

Beispiele:

*   `ingestion.env`: Chunk-Gr√∂√üe, Parallelisierung, Test-Limits
*   `processing.env`: Ein- und Ausgabepfade
*   `dashboard.env`: Titel und Beschreibung des Dashboards

√Ñnderungen an den `.env-Dateien` erfordern einen Neustart der Container:

```Bash
docker compose up -d --build
```

---

## Projektstruktur

*   `/ingestion`: Code f√ºr den ETL-Prozess (PGN -> Parquet).
*   `/processing`: Code f√ºr Aggregation und Feature Engineering.
*   `/dashboard`: Streamlit-Applikation.
*   `/env`: Konfiguration der Container.
*   `/data`: Lokaler Mount f√ºr den Data Lake (wird via .gitignore exkludiert).
*   `/logs`: Speicherort f√ºr persistente Log-Dateien der Services (`ingestion.log`, `processing.log` und `dashboard.log`) (wird via .gitignore exkludiert).

---

## Engineering-Konzepte

*   **Idempotenz**: Die Pipeline ist so konzipiert, dass sie beliebig oft neu gestartet werden kann. Zieldateien werden √ºberschrieben, sodass keine Duplikate entstehen.
*   **Skalierbarkeit (Partitioning)**: Der Ingestion-Service verarbeitet Dateien nicht "am St√ºck", sondern in Chunks (z.B. 10.000 Partien). Dies verhindert Memory-Overflows (OOM) und erm√∂glicht die Verarbeitung beliebig gro√üer Datens√§tze bei konstantem RAM-Verbrauch.
*   **Parallelisierung:** Der Ingestion-Prozess nutzt Python `multiprocessing` (ProcessPoolExecutor), um mehrere Quelldateien parallel auf allen verf√ºgbaren CPU-Kernen zu verarbeiten, was den Durchsatz signifikant erh√∂ht.
*   **Reliability**: Durch `service_completed_successfully` Conditions in Docker Compose wird sichergestellt, dass Services in der korrekten Reihenfolge starten (Vermeidung von Race Conditions).
*   **Observability (Logging):** Implementierung eines **Dual-Logging-Ansatzes**. Systemzust√§nde und Fehler werden sowohl in die Docker-Konsole (stdout) als auch persistent in rotierende Log-Dateien (`/logs`) geschrieben, um Debugging und Monitoring auch nach Container-Neustarts zu erm√∂glichen.
*   **Reproduzierbarkeit**: Alle Abh√§ngigkeiten sind in `requirements.txt` fixiert und laufen gemeinsam in isolierten Containern. Die Konfigurationsparameter sind in dedizierte `.env`-Dateien ausgelagert.
*   **Datenschutz**: Spielernamen werden w√§hrend der Ingestion verworfen (Datensparsamkeit).

---

## Ausblick: Machine Learning Integration

Die Entwicklung der eigentlichen **Machine Learning Applikation** (z. B. zur Vorhersage von Spielausg√§ngen) war **Out of Scope** f√ºr dieses Data-Engineering-Projekt. Die Architektur ist jedoch explizit darauf ausgelegt, als Backend f√ºr ML-Workflows zu dienen.

**Wie eine Integration aussehen k√∂nnte:**

Da der *Processing Service* bereits das Data Cleaning (Filterung, Typisierung) √ºbernimmt, kann ein Data Scientist direkt auf den **aufbereiteten Daten** aufsetzen, anstatt sich erneut mit den Rohdaten befassen zu m√ºssen.

**Beispiel-Workflow f√ºr ein Vorhersagemodell:**

1.  **Data Loading:** Das ML-Modell l√§dt die bereinigten Partitionen (z. B. aus einem "Silver Layer", bevor die Daten f√ºr das Dashboard aggregiert werden).
2.  **Training:**
    ```python
    # Pseudo-Code Beispiel mit Scikit-Learn
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier

    # Zugriff auf die vom Data Engineering vorbereiteten Daten
    # (Parquet ist performant und beh√§lt Datentypen bei)
    df = pd.read_parquet('data/processed/cleaned_games')
    
    # Training des Modells auf den sauberen Features
    X = df[['WhiteElo', 'BlackElo', 'ECO_Encoded']]
    y = df['Result']
    
    model = RandomForestClassifier()
    model.fit(X, y)
    ```
3.  **Deployment:** Das trainierte Modell k√∂nnte als vierter Container (z. B. mit **FastAPI** oder **MLflow**) in die `docker-compose`-Architektur integriert werden, um Vorhersagen f√ºr neue Partien in Echtzeit bereitzustellen.

---

## Lizenz


Dieses Projekt ist unter der **MIT Lizenz** lizenziert ‚Äì siehe die Datei [LICENSE](LICENSE) f√ºr Details.

Autor: Tom Maurer B.Sc.

Akademischer Kontext: Portfolio im Master-Modul Projekt: Data Engineering.



