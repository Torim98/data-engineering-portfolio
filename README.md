# Data Engineering Portfolio: Batch-Processing Pipeline f√ºr Schach-Analysen

Dieses Repository beinhaltet die Implementierung einer **Batch-basierten Datenarchitektur** im Rahmen des Portfolioprojekts "Data Engineering". Ziel des Projekts ist der Aufbau einer containerisierten Pipeline, die Schachpartien (PGN-Format) einliest, bereinigt, aggregiert und in einem Dashboard visualisiert.

Als Datenbasis dient die [Lichess Open Database](https://database.lichess.org/).

## üèóÔ∏è Architektur

Die Pipeline folgt einem Microservice-Ansatz und ist vollst√§ndig in **Docker** containerisiert. Sie implementiert eine "Medallion Architecture" (Bronze/Gold Layer) innerhalb eines lokalen Data Lakes.

**Die Komponenten:**

1.  **Ingestion Service:**
    *   Liest komprimierte Rohdaten (`.pgn.zst`).
    *   Implementiert Chunking/Partitioning: Verarbeitet Daten in konfigurierbaren Batches, um den RAM-Verbrauch konstant niedrig zu halten.
    *   Extrahiert Metadaten (Elo, Er√∂ffnung, Ergebnis) mittels `python-chess`.
    *   Speichert Rohdaten als partitionierte **Parquet**-Dateien (Bronze Layer).
2.  **Processing Service:**
    *   Wartet auf Abschluss der Ingestion.
    *   F√ºhrt Data Cleaning durch (Filterung ung√ºltiger Partien).
    *   Aggregiert Gewinnwahrscheinlichkeiten basierend auf Er√∂ffnungen (ECO-Codes).
    *   Speichert Ergebnisse im Gold Layer.
3.  **Dashboard Service:**
    *   Visualisiert die Ergebnisse mittels **Streamlit**.
    *   Erm√∂glicht explorative Datenanalyse im Browser.

---

## üöÄ Technologie-Stack

*   **Infrastructure as Code:** Docker & Docker Compose
*   **Sprache:** Python 3.11 (Slim Images)
*   **Datenformat:** Apache Parquet (Snappy Compression)
*   **Libraries:** Pandas, PyArrow, Python-Chess, Zstandard
*   **Frontend:** Streamlit

---

## üõ†Ô∏è Installation & Ausf√ºhrung

Voraussetzung: **Docker Desktop** muss installiert sein und laufen.

### 1. Repository klonen
```bash
git clone https://github.com/Torim98/data-engineering-portfolio.git
cd data-engineering-portfolio
```

### 2. Testdaten herunterladen

Aus Gr√ºnden der Speicherplatzoptimierung sind die Rohdaten nicht im Repository enthalten.

1. Lade eine Beispieldatei von database.lichess.org herunter (f√ºr Tests empfehlen sich kleinere Dateien von 2013/2014).
2. Platziere die Dateien im Ordner `data/`.

Die Ordnerstruktur sollte so aussehen:

```Text
/portfolio-chess-analytics
  ‚îú‚îÄ‚îÄ data/
  ‚îÇ   ‚îî‚îÄ‚îÄ lichess_sample1.pgn.zst
  ‚îÇ   ‚îî‚îÄ‚îÄ lichess_sample2.pgn.zst
  ‚îú‚îÄ‚îÄ ingestion/
  ‚îú‚îÄ‚îÄ processing/
  ‚îú‚îÄ‚îÄ dashboard/
  ‚îî‚îÄ‚îÄ docker-compose.yml
```

### 3. Pipeline starten

F√ºhre folgenden Befehl im Hauptverzeichnis aus:

```Bash
docker compose up --build
```

### 4. Dashboard √∂ffnen

Sobald die Pipeline durchgelaufen ist, ist das Dashboard unter folgender URL erreichbar:

üëâ http://localhost:8501

![Dashboard Vorschau](assets/dashboard_preview.png)

---

## üí° Engineering-Konzepte

*   **Idempotenz**: Die Pipeline ist so konzipiert, dass sie beliebig oft neu gestartet werden kann. Zieldateien werden √ºberschrieben, sodass keine Duplikate entstehen.
*   **Skalierbarkeit (Partitioning)**: Der Ingestion-Service verarbeitet Dateien nicht "am St√ºck", sondern in Chunks (z.B. 10.000 Partien). Dies verhindert Memory-Overflows (OOM) und erm√∂glicht die Verarbeitung beliebig gro√üer Datens√§tze bei konstantem RAM-Verbrauch.
*   **Reliability**: Durch `service_completed_successfully` Conditions in Docker Compose wird sichergestellt, dass Services in der korrekten Reihenfolge starten (Vermeidung von Race Conditions).
*   **Reproduzierbarkeit**: Alle Abh√§ngigkeiten sind in `requirements.txt` fixiert und laufen in isolierten Containern.
*   **Datenschutz**: Spielernamen werden w√§hrend der Ingestion verworfen (Datensparsamkeit).

---

## üìÇ Projektstruktur

*   `/ingestion`: Code f√ºr den ETL-Prozess (PGN -> Parquet).
*   `/processing`: Code f√ºr Aggregation und Feature Engineering.
*   `/dashboard`: Streamlit-Applikation.
*   `/data`: Lokaler Mount f√ºr den Data Lake (wird via .gitignore exkludiert).

---

## üîÆ Ausblick: Machine Learning Integration

Die Entwicklung der eigentlichen **Machine Learning Applikation** (z. B. zur Vorhersage von Spielausg√§ngen) war **Out of Scope** f√ºr dieses Data-Engineering-Projekt. Die Architektur ist jedoch explizit darauf ausgelegt, als Backend f√ºr ML-Workflows zu dienen.

**Wie eine Integration aussehen k√∂nnte:**

Da der *Processing Service* bereits das Data Cleaning (Filterung, Typisierung) √ºbernimmt, kann ein Data Scientist direkt auf den **aufbereiteten Daten** aufsetzen, anstatt sich erneut mit den Rohdaten befassen zu m√ºssen.

**Beispiel-Workflow f√ºr ein Vorhersagemodell:**

1.  **Data Loading:** Das ML-Modell l√§dt die bereinigten Partitionen (z. B. aus einem "Silver Layer", bevor die Daten f√ºr das Dashboard aggregiert werden).
2.  **Training:**
    ```python
    # Pseudo-Code Beispiel mit Scikit-Learn
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier

    # Zugriff auf die vom Data Engineering vorbereiteten Daten
    # (Parquet ist performant und beh√§lt Datentypen bei)
    df = pd.read_parquet('data/processed/cleaned_games')
    
    # Training des Modells auf den sauberen Features
    X = df[['WhiteElo', 'BlackElo', 'ECO_Encoded']]
    y = df['Result']
    
    model = RandomForestClassifier()
    model.fit(X, y)
    ```
3.  **Deployment:** Das trainierte Modell k√∂nnte als vierter Container (z. B. mit **FastAPI** oder **MLflow**) in die `docker-compose`-Architektur integriert werden, um Vorhersagen f√ºr neue Partien in Echtzeit bereitzustellen.

---

## üìÑ Lizenz

Dieses Projekt ist unter der **MIT Lizenz** lizenziert ‚Äì siehe die Datei [LICENSE](LICENSE) f√ºr Details.